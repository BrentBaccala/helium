# -*- mode: python -*-
#
# Python code to search for solutions of Hydrogen and Helium
# non-relativistic time-invariant Schrodinger equation
#
# INTERACTIVE USAGE:
#
# load('helium.sage')
# prep_hydrogen(5)
# init()
# ideal(eqns_RQQ).minimal_associated_primes()
#
# This will produce a solution to the hydrogen atom using ansatz 5.
# You can also prep_helium(), and supply an ansatz number as argument,
# as in prep_helium(-7).
#
# Negative ansatzen use a spherically symmetric Hamiltonian that
# reduces the dimension of the problem and lets us use the faster
# FLINT implementation because there are no roots in the Hamiltonian
# and FLINT doesn't have a Groebner basis implementation, which is
# required to handle roots.
#
# minimal_associated_primes() can run out of memory on large
# polynomial systems.  In the case, we can run a series of
# simplification steps that produce a family of simpler systems that
# are stored in a PostgreSQL database, processed in parallel, then
# recombined for the final result.  In this case, the command sequence
# looks like this:
#
# load('helium.sage')
# prep_hydrogen(5)
# init()
# create_database()
# SQL_stage1(eqns_RQQ)
# SQL_stage2()
# SQL_stage3_single_thread()
# GTZ_single_thread()
# consolidate_ideals(load_prime_ideals())
#
# SQL_stage3_single_thread() and GTZ_single_thread() have parallelized
# variants SQL_stage3_parallel() and GTZ_parallel().  Additionally,
# these routines can be run on a cluster, using the SQL database as a
# centralized data store.
#
# SQL_stage1() and SQL_stage2() run in parallel (multiple threads) on
# a single node.  SQL_stage2() can be run on multiple nodes in
# parallel.
#
# For speed, the SQL version of the code should be run with the
# espresso program compiled and available in the current working
# directory, and using a custom version of Sage that has an optimized
# simplifyIdeal function, available as the simplifyIdeal branch of
# https://github.com/BrentBaccala/sage.
#
# The two algorithms should produce identical results.  In particular,
# the output of the following two commands should be identical:
#
# sorted([j.groebner_basis() for j in ideal(eqns_RQQ).minimal_associated_primes()])
# sorted([j.groebner_basis() for j in consolidate_ideals(load_prime_ideals())])
#
# "Homogenization" (not a very good term) is used to force selected
# polynomials to be non-zero by forcing their coefficients to be 1,
# one at a time.  Different values of the 'homogenize' argument
# (starting at 0) force different coefficients to be 1, and an
# exception is thrown once all of the homogenization possibilities
# have been exhausted.  Specifying homogenize=-1 runs through
# all possible homogenizations (usually what you want).
#
# Homogenization can also be done when the trial solution is
# constructed (this is the 'homogenize' argument to the
# trial_polynomial function), which produces systems with fewer free
# coefficients, but a new system has to be constructed for every
# homogenization possibility, so I don't do it this way, and only plan
# to develop this option if computational complexity becomes an issue.
#
# CONCEPT:
#
# We have a differential equation that we're trying to solve and a
# trial solution with lots of free parameters that we try to adjust
# to get a solution.
#
# ALGORITHM:
#
# We start with a trial solution with free coefficients (coeff_vars)
# that we hope will yield an exact solution if they are set to the
# correct real numbers.  We don't try to reduce the complexity of this
# system to the point where we have single isolated solutions, so we
# expect our solutions to be higher-dimensional algebraic varieties.
#
# We plug the trial solution into the differential equation that we're
# trying to solve, expand it out, and collect coefficients of like
# terms.  This gives us a set of polynomial equations in the
# coefficients that define an algebraic variety in the space generated
# by the coefficient variables.  We find the minimal associated
# primes of the ideal generated by the polynomials, which gives
# us a decomposition of the solution space into irreducible varieties.
#
# by Brent Baccala
#
# first version - August 2019
# latest version - January 2025
#
# no rights reserved; you may freely copy, modify, or distribute this
# program

import itertools
from itertools import *

import subprocess

import threading

import pickle
import io

import os
import psutil
import datetime

import time

from sage.data_structures.bitset import FrozenBitset

import concurrent.futures

import traceback

# We expect to have twice as many threads as cores due to Intel Hyperthreading,
# so completely occupy the cores while leaving the other threads available.
num_processes = os.cpu_count() / 2

try:
    from clint.textui.progress import Bar
    # like Bar, but don't display ProgressBar at all if the whole event takes less than a second
    class ProgressBar(Bar):
        def __init__(self, label, expected_size):
            self.timer = threading.Timer(float(1.0), lambda: self.show(-1))
            self.timer.start()
            super().__init__(label=label, expected_size=expected_size, hide=True)
        def show(self, i):
            if i == -1:
                self.hide = False
                super().show(self.last_progress)
            else:
                super().show(i)
        def done(self):
            if self.timer:
                self.timer.cancel()
            else:
                super().done()
except ModuleNotFoundError:
    print("clint package not available; no progress bars will be displayed")
    class ProgressBar:
        def __init__(self, label, expected_size):
            pass
        def show(self, i):
            pass
        def done(self):
            pass

postgres_connection_parameters = {
    'user':     'baccala',
}

# If we're on my laptop (samsung, for testing) or the postgres server (edge), use localhost
# Otherwise, connect over IP to the postgres server (192.168.2.201, edge)
if not os.uname()[1] in ('samsung', 'edge'):
    postgres_connection_parameters['host'] = '192.168.2.201'

try:
    import psycopg2
except ModuleNotFoundError:
    print("psycopg2 package not available; no SQL database support")

def postgres_connect():
    try:
        global conn, conn2
        conn = psycopg2.connect(**postgres_connection_parameters)
        conn2 = psycopg2.connect(**postgres_connection_parameters)
    except psycopg2.OperationalError as ex:
        print('SQL OperationalError during connection attempt; no SQL database support')
    except NameError as ex:
        if ex.name == 'psycopg2':
            # if we couldn't load psycopg2, we already printed a warning
            pass
        else:
            raise

# from python docs
def flatten(listOfLists):
    "Flatten one level of nesting"
    return chain.from_iterable(listOfLists)

# from python docs
def powerset(iterable):
    "powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)"
    s = list(iterable)
    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))

def trial_polynomial(base, coordinates, roots, degree, homogenize=None, constant=True, first_index=0):
    """trial_polynomial(base, coordinates, roots, degree, homogenize=None, constant=True)
    Form a trial polynomial in the Symbolic Ring

    base is a string to which we append numbers to get our coefficient names; i.e, 'a' -> (a0,a1,a2,...)
    coordinates is a tuple of symbolic expressions (currently all symbols; i.e, x1.is_symbol() == True)
    roots is a tuple of symbolic expressions for our roots (currently all powers; i.e, r.operator() == pow) (and all square roots)
    degree is maximum degree of the trial polynomial
    constant=None is optional and drops the constant term (essentially mindeg=1 instead of mindeg=0)
    homogenize=N is unused right now and is intended as a future performance optimization

    The difference between 'coordinates' and 'roots' is that we never use higher powers of roots (>= 2) no matter
    what 'degree' is.  This assumes that the roots are square roots, of course, a current limitation of the code.
    """

    if not constant:
        mindegree = 1
    else:
        mindegree = 0
    terms = []
    for deg in range(mindegree, degree+1):
        terms += list(map(mul, (x for x in combinations_with_replacement(coordinates + roots, deg) if all(x.count(r) < 2 for r in roots))))

    coefficients = [var(base+str(c)) for c in range(first_index, first_index + len(terms))]
    poly_coefficients = list(coefficients)
    if homogenize != None:
        # homogenize: use 1 as the coefficient of the homogenize'th term
        #   and set all previous terms to 0.
        # The idea is to prevent a polynomial from being zero by running successive
        #   calculations running through all the terms of the polynomial, forcing them to be 1.
        if homogenize >= 0:
            homogenize_coefficient = homogenize % len(coefficients)
            if homogenize_coefficient > 0:
                poly_coefficients[0:homogenize_coefficient] = [0] * (homogenize_coefficient)
            poly_coefficients[homogenize_coefficient]= 1
            del coefficients[0:homogenize_coefficient+1]
        else:
            # if homogenize is negative, we set the coefficient of that term equal to 1,
            # but don't set all previous terms to 0.  I use this in ansatz 16.31
            homogenize_coefficient = -homogenize
            poly_coefficients[homogenize_coefficient]= 1
            del coefficients[homogenize_coefficient]

    poly = sum([poly_coefficients[c]*v for c,v in enumerate(terms)])
    return (tuple(coefficients), poly)

# Energy constant in Schroedinger's equations
var('E')

def Del(Psi,vars):
    return sum([diff(Psi,v,2) for v in vars])

# Create an operator (DD) that takes derivatives of symbolic functions
# w.r.t. their arguments, i.e, `DD[0](Phi)(A)` is the derivative of
# Phi w.r.t. its 0-th argument, `A`.
#
# from https://groups.google.com/g/sage-devel/c/xBHw11qUARg/m/0eqj3eUFsFkJ
# referenced from https://trac.sagemath.org/ticket/17445

from sage.symbolic.operators import FDerivativeOperator
class Doperator:
  def __init__(self,vars=None):
    self.vars= [] if vars is None else vars

  def __call__(self,f):
    return FDerivativeOperator(f,self.vars)

  def __getitem__(self,i):
    if isinstance(i,tuple):
       newvars=self.vars+list(i)
    else:
       newvars=self.vars+[i]
    return Doperator(newvars)

DD=Doperator()

def finish_prep(ansatz):
    global eq, H, coeff_vars, ODE_vars, coordinates, roots
    global A,B,C,D,F,G,M,N,V
    global homogenize_groups
    global alg_exts
    global gamma

    (Avars, A) = trial_polynomial('a', coordinates, roots, 1)
    (Bvars, B) = trial_polynomial('b', coordinates, roots, 1)
    (Cvars, C) = trial_polynomial('c', coordinates, roots, 1)
    (Dvars, D) = trial_polynomial('d', coordinates, roots, 1)
    (Fvars, F) = trial_polynomial('f', coordinates, roots, 1)
    (Gvars, G) = trial_polynomial('g', coordinates, roots, 1)

    SR_function = sage.symbolic.function_factory.function

    alg_exts = tuple()

    if ansatz == 1:
        # A linear polynomial times the exponential of a linear polynomial
        # Phi is an exponential of a linear polynomial
        # Phi = e^B, so diff(Phi,B) = Phi and diff(Phi,v) = diff(B,v)*Phi
        # A is a linear polynomial; the solution is A times Phi.
        # Homogenization forces A to be non-zero.
        #
        # An earlier verion of this ansatz was used extensively for testing Hydrogen
        Phi = SR_function('Phi')
        (Avars, A) = trial_polynomial('a', coordinates, roots, 1)
        (Bvars, B) = trial_polynomial('b', coordinates, roots, 1)
        Psi = A * Phi(B)
        # subs is a list of dictionaries defining substitutions.  They are executed in order.
        subs = [{DD[0](Phi)(B) : Phi(B), DD[0,0](Phi)(B) : Phi(B)},
                {Phi(B) : SR.var('Phi')}
        ]
        homogenize_groups = (Avars, Bvars)
        coeff_vars = (E,) + Avars + Bvars
        ODE_vars = ('Phi', )

    elif ansatz == 2:
        # A linear polynomial times the logarithm of a linear polynomial
        # Xi is a logarithm; Xi = ln C, so diff(Xi,C) = 1/C and diff(Xi,v) = diff(C,v)/C
        # A is a linear polynomial; the solution is A times Xi.
        # Homogenization forces A and C to be non-zero
        #
        # I've used this ansatz very little.
        Xi = SR_function('Xi')
        Psi = A * Xi(C)
        subs = [{DD[0](Xi)(C) : 1/C, DD[0,0](Xi)(C) : -1/C^2},
                {Xi(C) : SR.var('Xi')}
        ]
        homogenize_groups = (Avars, Cvars)
        coeff_vars = (E,) + Avars + Cvars
        ODE_vars = ('Xi', )

    elif ansatz == 3:
        # Chi is a weird second-order mess: C d^2 Chi/dB^2 - D dChi/dB - F Chi - G = 0
        # A is a linear polynomial; the solution is A times Chi.
        #
        # I declared Chi to be a function of B, but in retrospect, it's also a
        # function of C, D, F, and G, none of which are (necessarily) functions of B
        #
        # Homogenization forces A, B and C to be non-zero, and B is non-constant.
        #
        # I've used this ansatz very little; ansatz 4 is closely related
        Chi = SR_function('Chi')
        (Bvars, B) = trial_polynomial('b', coordinates, roots, 1, constant=None)
        Psi = A*Chi(B)

        homogenize_groups = (Avars, Bvars, Cvars)
        coeff_vars = (E,) + Avars + Bvars + Cvars + Dvars + Fvars + Gvars

        subs = [{DD[0,0](Chi)(B) : (D/C * DD[0](Chi)(B) + F/C * Chi(B)) + G/C},
                {Chi(B) : SR.var('Chi'), DD[0](Chi)(B) : SR.var('DChi')}
        ]
        ODE_vars = ('Chi', 'DChi')

    elif ansatz == 4:
        # Like ansatz 3, but without the polynomial A as a factor, and thus simplier
        #
        # Homogenization forces B and C to be non-zero, and B is non-constant.
        #
        # This ansatz is fairly well explored, but in an earlier version of the code
        # (pre-edbaa8 and pre-e74ded) whose ring and class structures aren't compatible.
        Chi = SR_function('Chi')
        (Bvars, B) = trial_polynomial('b', coordinates, roots, 1, constant=None)
        Psi = Chi(B)

        homogenize_groups = (Bvars, Cvars)
        coeff_vars = (E,) + Bvars + Cvars + Dvars + Fvars + Gvars

        subs = [{DD[0,0](Chi)(B) : (D/C * DD[0](Chi)(B) + F/C * Chi(B)) + G/C},
                {Chi(B) : SR.var('Chi'), DD[0](Chi)(B) : SR.var('DChi')}
        ]
        ODE_vars = ('Chi', 'DChi')

    elif ansatz == 5 or ansatz == 5.01:
        # A second-order homogeneous ODE: D(V) d^2 Zeta/dV^2 + M(V) dZeta/dV + N(V) Zeta = 0
        # where D(V), M(V), and N(V) are linear polynomials in V, which is itself a linear polynomial
        #
        # Homogenization forces V and D to be non-zero; V is also forced to be non-constant
        Zeta = SR_function('Zeta')
        (Vvars, V) = trial_polynomial('v', coordinates, roots, 1, constant=None, first_index=1)
        Psi = Zeta(V)
        if ansatz == 5.01:
            # use 'homogenize' to set the coeffient of v in the ODE's second order coefficient to 1
            (Avars, A) = trial_polynomial('a', [V], [], 1, homogenize=-1)
        else:
            (Avars, A) = trial_polynomial('a', [V], [], 1)
        (Bvars, B) = trial_polynomial('b', [V], [], 1)
        (Cvars, C) = trial_polynomial('c', [V], [], 1)

        homogenize_groups = (Avars, Vvars)

        coeff_vars = (E,) + Vvars + Avars + Bvars + Cvars

        subs = [{DD[0,0](Zeta)(V) : -(B * DD[0](Zeta)(V) + C * Zeta(V)) / A},
                {Zeta(V) : SR.var('Zeta'), DD[0](Zeta)(V) : SR.var('DZeta')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

    elif ansatz == 5.1:
        # A second-order homogeneous ODE: D(V) d^2 Zeta/dV^2 - M(V) dZeta/dV - N(V) Zeta = 0
        # where D(V), M(V), and N(V) are linear polynomials in V, which is a quadratic polynomial
        #
        # Homogenization forces V and D to be non-zero; V is also forced to be non-constant
        Zeta = SR_function('Zeta')
        (Vvars, V) = trial_polynomial('v', coordinates, roots, 2, constant=None)
        Psi = Zeta(V)
        (Dvars, D) = trial_polynomial('d', [V], [], 1)
        (Mvars, M) = trial_polynomial('m', [V], [], 1)
        (Nvars, N) = trial_polynomial('n', [V], [], 1)

        homogenize_groups = (Dvars, Vvars)

        coeff_vars = (E,) + Vvars + Dvars + Mvars + Nvars

        subs = [{DD[0,0](Zeta)(V) : (M * DD[0](Zeta)(V) + N * Zeta(V)) / D},
                {Zeta(V) : SR.var('Zeta'), DD[0](Zeta)(V) : SR.var('DZeta')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

    elif ansatz == 5.2:
        # A second-order homogeneous ODE: D(V) d^2 Zeta/dV^2 - M(V) dZeta/dV - N(V) Zeta = 0
        # where D(V), M(V), and N(V) are quadratic polynomials in V, which is a linear polynomial
        #
        # Homogenization forces V and D to be non-zero; V is also forced to be non-constant
        Zeta = SR_function('Zeta')
        (Vvars, V) = trial_polynomial('v', coordinates, roots, 1, constant=None)
        Psi = Zeta(V)
        (Dvars, D) = trial_polynomial('d', [V], [], 2)
        (Mvars, M) = trial_polynomial('m', [V], [], 2)
        (Nvars, N) = trial_polynomial('n', [V], [], 2)

        homogenize_groups = (Dvars, Vvars)

        coeff_vars = (E,) + Vvars + Dvars + Mvars + Nvars

        subs = [{DD[0,0](Zeta)(V) : (M * DD[0](Zeta)(V) + N * Zeta(V)) / D},
                {Zeta(V) : SR.var('Zeta'), DD[0](Zeta)(V) : SR.var('DZeta')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

    elif ansatz == 5.3:
        # A second-order homogeneous ODE: D(V) d^2 Zeta/dV^2 - M(V) dZeta/dV - N(V) Zeta = 0
        # where D(V), M(V), and N(V) are quadratic polynomials in V, which is also a quadratic polynomial
        #
        # Homogenization forces V and D to be non-zero; V is also forced to be non-constant
        Zeta = SR_function('Zeta')
        (Vvars, V) = trial_polynomial('v', coordinates, roots, 2, constant=None)
        Psi = Zeta(V)
        (Dvars, D) = trial_polynomial('d', [V], [], 2)
        (Mvars, M) = trial_polynomial('m', [V], [], 2)
        (Nvars, N) = trial_polynomial('n', [V], [], 2)

        homogenize_groups = (Dvars, Vvars)

        coeff_vars = (E,) + Vvars + Dvars + Mvars + Nvars

        subs = [{DD[0,0](Zeta)(V) : (M * DD[0](Zeta)(V) + N * Zeta(V)) / D},
                {Zeta(V) : SR.var('Zeta'), DD[0](Zeta)(V) : SR.var('DZeta')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

    elif ansatz == 6:
        # A second-order homogeneous ODE: D(B/C) d^2 Zeta/d(B/C)^2 - M(B/C) dZeta/d(B/C) - N(B/C) Zeta = 0
        # where D(B/C), M(B/C), and N(B/C) are linear polynomials in B/C, a first-degree rational function
        Zeta = SR_function('Zeta')
        Psi = Zeta(B/C)
        (Dvars, D) = trial_polynomial('d', [B/C], [], 1)
        (Mvars, M) = trial_polynomial('m', [B/C], [], 1)
        (Nvars, N) = trial_polynomial('n', [B/C], [], 1)

        coeff_vars = (E,) + Bvars + Cvars + Dvars + Mvars + Nvars

        subs = [{DD[0,0](Zeta)(B/C) : (M * DD[0](Zeta)(B/C) + N * Zeta(B/C)) / D},
                {Zeta(B/C) : SR.var('Zeta'), DD[0](Zeta)(B/C) : SR.var('DZeta')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

    elif ansatz == 7:
        # A second-order homogeneous ODE: D(B/C) d^2 Zeta/d(B/C)^2 - M(B/C) dZeta/d(B/C) - N(B/C) Zeta = 0
        # where D(B/C), M(B/C), and N(B/C) are second-degree polynomials in B/C, a second-degree rational function
        Zeta = SR_function('Zeta')
        (Bvars, B) = trial_polynomial('b', coordinates, roots, 2, homogenize=0)
        (Cvars, C) = trial_polynomial('c', coordinates, roots, 2, homogenize=1)
        Psi = Zeta(B/C)
        (Dvars, D) = trial_polynomial('d', [B/C], [], 2, homogenize=0)
        (Mvars, M) = trial_polynomial('m', [B/C], [], 2)
        (Nvars, N) = trial_polynomial('n', [B/C], [], 2)

        coeff_vars = (E,) + Bvars + Cvars + Dvars + Mvars + Nvars

        subs = [{DD[0,0](Zeta)(B/C) : (M * DD[0](Zeta)(B/C) + N * Zeta(B/C)) / D},
                {Zeta(B/C) : SR.var('Zeta'), DD[0](Zeta)(B/C) : SR.var('DZeta')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

    elif ansatz == 8:
        # A first-order homogeneous ODE: M(B) dZeta/dB - N(B) Zeta = 0
        # where M(B) and N(B) are linear polynomials in B, which is itself a linear polynomial
        # B can not be constant; neither B or M can be zero (homogenization)
        #
        # Logically it's a step backwards from ansatz 5, but I want to see it work.

        Zeta = SR_function('Zeta')
        (Bvars, B) = trial_polynomial('b', coordinates, roots, 1, constant=None)
        Psi = Zeta(B)
        (Mvars, M) = trial_polynomial('m', [B], [], 1)
        (Nvars, N) = trial_polynomial('n', [B], [], 1)

        homogenize_groups = (Mvars, Bvars)
        coeff_vars = (E,) + Bvars + Mvars + Nvars

        # A limitation of the program is that I have to manually calculate DD[0,0](Zeta)(B) here
        #  DD[0,0](Zeta)(B)  = d^2 Zeta / dB^2 = d/dB (N(B) * Zeta(B) / M(B))
        #     = (dN/dB * Zeta(B) * M(B) + N(B) * DD[0](Zeta)(B) * M(B) - N(B) * Zeta(B) * dM/dB ) / M^2(B)
        #     = (n1 * Zeta(B) * M(B) + N(B) * DD[0](Zeta)(B) * M(B) - N(B) * Zeta(B) * m1 ) / M^2(B)
        #
        # Can't write diff(M,B) because B is a polynomial and diff only accepts a symbol as its second argument.
        # Yet we know that M = m1*B + m0, so diff(M,B)=m1
        m1 = Mvars[1]
        n1 = Nvars[1]
        subs = [{DD[0](Zeta)(B) : (N * Zeta(B)) / M,
                 DD[0,0](Zeta)(B) : (n1 * Zeta(B) * M + N * N * Zeta(B) - N * Zeta(B) * m1 ) / (M*M) },
                {Zeta(B) : SR.var('Zeta')}
        ]
        ODE_vars = ('Zeta', )

    elif ansatz == 9:
        # A first-order homogeneous ODE: dZeta/dB - n0 Zeta = 0
        # where n0 is a constant and B is a linear polynomial
        #
        # Logically it's a further step backwards from ansatz 8, but ansatz 8 has too many free variables
        # for scipy.optimize.root to work on 1-dim hydrogen if we use homogenization
        Zeta = SR_function('Zeta')
        (Bvars, B) = trial_polynomial('b', coordinates, roots, 1, constant=None)
        Psi = Zeta(B)
        (Nvars, N) = trial_polynomial('n', [B], [], 0)

        homogenize_groups = (Bvars, )
        coeff_vars = (E,) + Bvars + Nvars

        # A limitation of the program is that I have to manually calculate DD[0,0](Zeta)(B) here
        #  DD[0](Zeta)(B)  = n0 Zeta(B)
        #  DD[0,0](Zeta)(B)  = n0^2 Zeta(B)

        n0 = Nvars[0]
        subs = [{DD[0](Zeta)(B) : n0 * Zeta(B),
                 DD[0,0](Zeta)(B) : n0^2 * Zeta(B)},
                {Zeta(B) : SR.var('Zeta')}
        ]
        ODE_vars = ('Zeta', )

    elif ansatz == 10:
        # A second-order homogeneous ODE: D(B) d^2 Zeta/dB^2 - M(B) dZeta/dB - N(B) Zeta = 0
        # where D(B), M(B), and N(B) are quadratic polynomials in B, which is itself a quadratic polynomial
        #
        # Homogenization forces B and D to be non-zero; B is also forced to be non-constant
        Zeta = SR_function('Zeta')
        (Bvars, B) = trial_polynomial('b', coordinates, roots, 2, constant=None)
        Psi = Zeta(B)
        (Dvars, D) = trial_polynomial('d', [B], [], 2)
        (Mvars, M) = trial_polynomial('m', [B], [], 2)
        (Nvars, N) = trial_polynomial('n', [B], [], 2)

        homogenize_groups = (Dvars, Bvars)

        coeff_vars = (E,) + Bvars + Dvars + Mvars + Nvars

        subs = [{DD[0,0](Zeta)(B) : (M * DD[0](Zeta)(B) + N * Zeta(B)) / D},
                {Zeta(B) : SR.var('Zeta'), DD[0](Zeta)(B) : SR.var('DZeta')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

    elif ansatz == 11:
        # A second-degree algebraic extension (linear coeffs) followed by
        # a second-order homogeneous ODE: D(V) d^2 Zeta/dV^2 - M(V) dZeta/dV - N(V) Zeta = 0
        # where D(V), M(V), and N(V) are linear polynomials in V, which is itself a linear polynomial
        #
        # Homogenization forces V and D to be non-zero; V is also forced to be non-constant

        (Avars, A) = trial_polynomial('a', coordinates, roots, 1)
        (Bvars, B) = trial_polynomial('b', coordinates, roots, 1)
        (Cvars, C) = trial_polynomial('c', coordinates, roots, 1)
        def deriv(self, *args,**kwds):
            #print("{} {} {}".format(self, args, kwds))
            wrt = args[kwds['diff_param']]
            return -(diff(A, wrt)*self(*coordinates)^2+diff(B,wrt)*self(*coordinates)+diff(C,wrt)/(2*A*self(*coordinates)+B))
        # anything that isn't constant w.r.t. coordinates is an SR_function
        gamma = SR_function('g', nargs=len(coordinates), derivative_func=deriv)

        # We can construct derivatives like this, too:
        # sage: DD[0](gamma)(x1,y1,z1)
        # diff(g(x1, y1, z1), x1)
        # sage: DD[1](gamma)(x1,y1,z1)
        # diff(g(x1, y1, z1), y1)
        # sage: DD[1,1](gamma)(x1,y1,z1)
        # diff(g(x1, y1, z1), y1, y1)

        Zeta = SR_function('Zeta')
        (Vvars, V) = trial_polynomial('v', coordinates, roots + (gamma(*coordinates),), 1, constant=None)
        Psi = Zeta(V)
        (Dvars, D) = trial_polynomial('d', [V], [], 1)
        (Mvars, M) = trial_polynomial('m', [V], [], 1)
        (Nvars, N) = trial_polynomial('n', [V], [], 1)

        homogenize_groups = (Dvars, Vvars)

        coeff_vars = (E,) + Vvars + Dvars + Mvars + Nvars + Avars + Bvars + Cvars
        print(coeff_vars)

        subs = [{DD[0,0](Zeta)(V) : (M * DD[0](Zeta)(V) + N * Zeta(V)) / D},
                {Zeta(V) : SR.var('Zeta'), DD[0](Zeta)(V) : SR.var('DZeta')},
                {gamma(*coordinates) : SR.var('g')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

        alg_exts = (('g', A*gamma(*coordinates)^2 + B*gamma(*coordinates) + C, subs[2]),)

    elif int(ansatz) == 12:
        # A second-degree homogenous ODE (linear coeffs) followed by another
        # second-order homogeneous ODE: D(V) d^2 Zeta/dV^2 - M(V) dZeta/dV - N(V) Zeta = 0
        # where D(V), M(V), and N(V) are linear polynomials in V, which is itself a linear polynomial
        #
        # Homogenization forces V and D to be non-zero; V is also forced to be non-constant

        # trial_polynomial returns a tuple: the coefficient variables used, and the polynomial itself

        if ansatz == 12:
            maxdeg_v = 1
            maxdeg_u = 1
            maxdeg_ode_v = 1
            maxdeg_ode_u = 1
        elif ansatz == 12.1:
            maxdeg_v = 2
            maxdeg_u = 1
            maxdeg_ode_v = 1
            maxdeg_ode_u = 1
        elif ansatz == 12.2:
            maxdeg_v = 1
            maxdeg_u = 2
            maxdeg_ode_v = 1
            maxdeg_ode_u = 1
        elif ansatz == 12.3:
            maxdeg_v = 1
            maxdeg_u = 1
            maxdeg_ode_v = 2
            maxdeg_ode_u = 1
        elif ansatz == 12.4:
            maxdeg_v = 1
            maxdeg_u = 1
            maxdeg_ode_v = 1
            maxdeg_ode_u = 2

        Theta = SR_function('Theta')
        (Uvars, U) = trial_polynomial('u', coordinates, roots, maxdeg_u, constant=None)
        (Avars, A) = trial_polynomial('a', [U], [], maxdeg_ode_u)
        (Bvars, B) = trial_polynomial('b', [U], [], maxdeg_ode_u)
        (Cvars, C) = trial_polynomial('c', [U], [], maxdeg_ode_u)

        Zeta = SR_function('Zeta')
        (Vvars, V) = trial_polynomial('v', coordinates + (Theta(U),), roots, maxdeg_v, constant=None)

        (Dvars, D) = trial_polynomial('d', [V], [], maxdeg_ode_v)
        (Mvars, M) = trial_polynomial('m', [V], [], maxdeg_ode_v)
        (Nvars, N) = trial_polynomial('n', [V], [], maxdeg_ode_v)

        # Psi is the solution to the PDE
        Psi = Zeta(V)

        homogenize_groups = (Dvars, Vvars)

        coeff_vars = (E,) + Uvars + Avars + Bvars + Cvars + Vvars + Dvars + Mvars + Nvars
        print(coeff_vars)

        # Make sure Zeta(V)->SR(Zeta) comes before Theta(U)->SR(Theta) because
        # Zeta(V) will have Theta(U) in its arguments and the Theta(U) sub screws up that match
        subs = [{DD[0,0](Theta)(U) : (B * DD[0](Theta)(U) + C * Theta(U)) / A},
                {DD[0,0](Zeta)(V) : (M * DD[0](Zeta)(V) + N * Zeta(V)) / D},
                {Zeta(V) : SR.var('Zeta'), DD[0](Zeta)(V) : SR.var('DZeta')},
                {Theta(U) : SR.var('Theta'), DD[0](Theta)(U) : SR.var('DTheta')}
        ]

        # It does need a list of the additional variables for when it builds the polynomial ring
        # Probably it could just infer this information from the variables present in the equation
        ODE_vars = ('Theta', 'DTheta', 'Zeta', 'DZeta')

    elif int(ansatz) == 13:
        # A second-degree algebraic extension (linear coeffs) used as the coefficient ring
        # in a second-order homogeneous ODE: D(V) d^2 Zeta/dV^2 - M(V) dZeta/dV - N(V) Zeta = 0
        # where D(V), M(V), and N(V) are linear polynomials in V (a linear polynomial) and gamma.
        #
        # Homogenization forces V and D to be non-zero; V is also forced to be non-constant

        if ansatz == 13:
            maxdeg_v = 1
            maxdeg_alg = 1
            maxdeg_ode = 1
        elif ansatz == 13.1:
            maxdeg_v = 2
            maxdeg_ode = 1
            maxdeg_alg = 1
        elif ansatz == 13.2:
            maxdeg_v = 1
            maxdeg_ode = 2
            maxdeg_alg = 1
        elif ansatz == 13.3:
            maxdeg_v = 1
            maxdeg_ode = 1
            maxdeg_alg = 2
        elif ansatz == 13.4:
            maxdeg_v = 2
            maxdeg_ode = 2
            maxdeg_alg = 1
        elif ansatz == 13.5:
            maxdeg_v = 2
            maxdeg_ode = 1
            maxdeg_alg = 2
        elif ansatz == 13.5:
            maxdeg_v = 1
            maxdeg_ode = 2
            maxdeg_alg = 2
        elif ansatz == 13.6:
            maxdeg_v = 2
            maxdeg_ode = 2
            maxdeg_alg = 2

        Zeta = SR_function('Zeta')
        (Vvars, V) = trial_polynomial('v', coordinates, roots, maxdeg_v, constant=None)

        (Avars, A) = trial_polynomial('a', [V], [], maxdeg_alg)
        (Bvars, B) = trial_polynomial('b', [V], [], maxdeg_alg)
        (Cvars, C) = trial_polynomial('c', [V], [], maxdeg_alg)
        def deriv(self, *args,**kwds):
            #print("{} {} {}".format(self, args, kwds))
            return -(diff(A, V)*self(*coordinates)^2+diff(B,V)*self(*coordinates)+diff(C,V)/(2*A*self(*coordinates)+B))
        # anything that isn't constant w.r.t. coordinates is an SR_function
        gamma = SR_function('g', nargs=1, derivative_func=deriv)

        Psi = Zeta(V)
        (Dvars, D) = trial_polynomial('d', [V], [gamma(V)], maxdeg_ode)
        (Mvars, M) = trial_polynomial('m', [V], [gamma(V)], maxdeg_ode)
        (Nvars, N) = trial_polynomial('n', [V], [gamma(V)], maxdeg_ode)

        homogenize_groups = (Dvars, Vvars)

        coeff_vars = (E,) + Vvars + Dvars + Mvars + Nvars + Avars + Bvars + Cvars
        print(coeff_vars)

        subs = [{DD[0,0](Zeta)(V) : (M * DD[0](Zeta)(V) + N * Zeta(V)) / D},
                {Zeta(V) : SR.var('Zeta'), DD[0](Zeta)(V) : SR.var('DZeta')},
                {gamma(V) : SR.var('g')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

        #alg_exts = (('g', A*gamma(V)^2 + B*gamma(V) + C, post2_subs),)
        alg_exts = (('g', A*SR.var('g')^2 + B*SR.var('g') + C, subs[2]),)

    elif int(ansatz) == 16:
        # A second-degree algebraic extension (root of a linear polynomial) followed by
        # a second-order homogeneous ODE: D(V) d^2 Zeta/dV^2 - M(V) dZeta/dV - N(V) Zeta = 0
        # where D(V), M(V), and N(V) are linear polynomials in V, which is itself a linear polynomial
        #
        # Homogenization forces V and D to be non-zero; V is also forced to be non-constant

        if ansatz == 16:
            maxdeg_v = 1
            maxdeg_alg = 1
            maxdeg_ode = 1
        elif ansatz == 16.1:
            maxdeg_v = 2
            maxdeg_ode = 1
            maxdeg_alg = 1
        elif ansatz == 16.2:
            maxdeg_v = 1
            maxdeg_ode = 2
            maxdeg_alg = 1
        elif ansatz == 16.3 or ansatz == 16.31:
            maxdeg_v = 1
            maxdeg_ode = 1
            maxdeg_alg = 2
        elif ansatz == 16.4:
            maxdeg_v = 2
            maxdeg_ode = 2
            maxdeg_alg = 1
        elif ansatz == 16.5:
            maxdeg_v = 2
            maxdeg_ode = 1
            maxdeg_alg = 2
        elif ansatz == 16.5:
            maxdeg_v = 1
            maxdeg_ode = 2
            maxdeg_alg = 2
        elif ansatz == 16.6 or ansatz == 16.61:
            maxdeg_v = 2
            maxdeg_ode = 2
            maxdeg_alg = 2

        (Avars, A) = trial_polynomial('a', coordinates, roots, maxdeg_alg)
        def deriv(self, *args,**kwds):
            wrt = args[kwds['diff_param']]
            return diff(A, wrt)/(2*A*self(*coordinates))
        # anything that isn't constant w.r.t. coordinates is an SR_function
        gamma = SR_function('g', nargs=len(coordinates), derivative_func=deriv)

        # We can construct derivatives like this, too:
        # sage: DD[0](gamma)(x1,y1,z1)
        # diff(g(x1, y1, z1), x1)
        # sage: DD[1](gamma)(x1,y1,z1)
        # diff(g(x1, y1, z1), y1)
        # sage: DD[1,1](gamma)(x1,y1,z1)
        # diff(g(x1, y1, z1), y1, y1)

        Zeta = SR_function('Zeta')
        if ansatz == 16.31 or ansatz == 16.61:
            # use 'homogenize' to set the coefficient of gamma to 1
            (Vvars, V) = trial_polynomial('v', coordinates, (gamma(*coordinates),) + roots, maxdeg_v, constant=None, homogenize=0)
        else:
            (Vvars, V) = trial_polynomial('v', coordinates, roots + (gamma(*coordinates),), maxdeg_v, constant=None)
        Psi = Zeta(V)
        if ansatz == 16.31 or ansatz == 16.61:
            # use 'homogenize' to set the coeffient of v in the ODE's second order coefficient to 1
            (Dvars, D) = trial_polynomial('d', [V], [], maxdeg_ode, homogenize=-1)
        else:
            (Dvars, D) = trial_polynomial('d', [V], [], maxdeg_ode)
        (Mvars, M) = trial_polynomial('m', [V], [], maxdeg_ode)
        (Nvars, N) = trial_polynomial('n', [V], [], maxdeg_ode)

        homogenize_groups = (Dvars, Vvars)

        coeff_vars = (E,) + Vvars + Dvars + Mvars + Nvars + Avars
        print(coeff_vars)

        subs = [{DD[0,0](Zeta)(V) : (M * DD[0](Zeta)(V) + N * Zeta(V)) / D},
                {Zeta(V) : SR.var('Zeta'), DD[0](Zeta)(V) : SR.var('DZeta')},
                {gamma(*coordinates) : SR.var('g')}
        ]
        ODE_vars = ('Zeta', 'DZeta')

        # alg_exts is a list of tuples
        # each tuple is (name, minimal polynomial, substitution)
        # minimal polynomial has its roots converted to rs, then subsitution is applied, then append to ideal to mod out by
        alg_exts = (('g', gamma(*coordinates)^2 - A, subs[2]),)

    else:
        raise 'Bad ansatz'

    eq = H(Psi) - E*Psi

    for sub in subs:
        eq = eq.subs(sub)

    # reduce coeff_vars to those which actually appear in the equation
    # let's not do this, in case we've got algebraic extension elements (like ansatz 11)
    # coeff_vars = tuple(sorted(set(eq.free_variables()).intersection(coeff_vars), key=lambda x:str(x)))

    # I used to do this in convert_eq_a(), but that function can be slow, and this is pretty quick,
    # so let's put it in the "prep()" step instead of the "init()" step
    create_polynomial_rings(alg_exts)
    create_eq_a()

def prep_hydrogen(ansatz=1):
    global H, coordinates, roots
    global r

    if ansatz < 0:

        var('r')
        coordinates = (r,)
        roots = tuple()

        def H(Psi):
            return - 1/2 * (1/r^2 * diff(r^2 * diff(Psi,r), r)) - (1/r)*Psi

    else:
        var('x,y,z')
        coordinates = (x,y,z)

        r = sqrt(x^2+y^2+z^2)
        roots = (r,)

        def H(Psi):
            return - 1/2 * Del(Psi,[x,y,z]) - (1/r)*Psi

    postgres_connection_parameters['database'] = 'hydrogen-' + str(abs(ansatz))
    postgres_connect()

    finish_prep(ansatz=abs(ansatz))

def prep_helium(ansatz=6):
    global H, coordinates, roots

    if ansatz < 0:
        # According to Nakatsuji, we can write the helium Hamiltonian
        # for S states (no angular momentum) in a r1/r2/r12 coordinate system.
        #
        # That's what we do for a negative ansatz

        var('R1,R2,R12')
        coordinates = (R1,R2,R12)
        roots = tuple()

        # eq (5) in Nakashima and Nakatusji, Solving the Schrodinger equation for helium...
        # THE JOURNAL OF CHEMICAL PHYSICS 127, 224104 2007
        def H(Psi):
            return - 1/2 *sum(diff(Psi, Ri, 2) + 2/Ri*diff(Psi,Ri) for Ri in [R1,R2])  \
                   - (diff(Psi, R12, 2) + 2/R12*diff(Psi,R12))                          \
                   - (R1^2 + R12^2 - R2^2)/(2*R1*R12) * diff(diff(Psi,R12),R1)         \
                   - (R2^2 + R12^2 - R1^2)/(2*R2*R12) * diff(diff(Psi,R12),R2)         \
                   - sum(2/Ri for Ri in [R1,R2])*Psi + 1/R12*Psi

    else:

        var('x1,y1,z1,x2,y2,z2')

        global r1, r2, r12
        r1 = sqrt(x1^2+y1^2+z1^2)
        r2 = sqrt(x2^2+y2^2+z2^2)
        r12 = sqrt((x2-x1)^2 + (y2-y1)^2 + (z2-z1)^2)

        coordinates = (x1,y1,z1, x2,y2,z2)
        roots = (r1,r2,r12)

        def H(Psi):
            return - 1/2 * Del(Psi,[x1,y1,z1]) - 1/2 * Del(Psi,[x2,y2,z2]) - (2/r1)*Psi - (2/r2)*Psi + (1/r12)*Psi

    postgres_connection_parameters['database'] = 'helium-' + str(round(abs(ansatz), 1))
    postgres_connect()

    finish_prep(ansatz=abs(ansatz))


# Now we want to replace all of the sqrt(...) factors with 'r',
# and we use a clever Python trick to build a dictionary
# that maps expressions to variable names.

def varName(var):
    for name,value in globals().items():
        if id(var) == id(value):
            return name
    return None

def mk_maps(roots):
    return {v.operands()[0] : SR.var(varName(v)) for v in roots}

# convert all (x^2+y^2+z^2)^(n/2) expressions to r^n
# What if we have multiple x^2+y^2+z^2 expressions in a single power?
def roots_to_rs(expr):
    if isinstance(expr, Expression) and expr.operator():
       if expr.operator() == operator.pow and bool(expr.operands()[0] in maps):
           return maps[expr.operands()[0]]^(expr.operands()[1] * 2)
       else:
           return expr.operator()(*map(roots_to_rs, expr.operands()))
    else:
       return expr

def create_eq_a():
    # first, build the dictionary that maps expressions like (x1^2+y1^2+z1^2) to variables like r1
    # make 'maps' global to simplify the map function inside roots_to_rs()
    global maps, eq_a
    maps = mk_maps(roots)
    # next, convert all of the roots in the equation to use the r-variables
    eq_a = roots_to_rs(eq)

# Print "equation A" in the form parsed by GNU EMACS's outline mode.
# This function is only used for debugging.

def analyze_eq_a(eq, depth=1, print_depth=2, file=sys.stdout):
    if eq.operator():
       print('*' * depth, eq.operator(), len(eq.operands()), file=file)
       if depth >= print_depth: print(eq, file=file)
       for o in eq.operands():
           analyze_eq_a(o, depth+1, print_depth=print_depth, file=file)


# Create a polynomial ring to hold our expressions.
#
# Sage does this using Singular, which stores polynomials internally in standard
# form (i.e, fully expanded).  Singular is the default, while FLINT is available
# as an option, which I tend to use because I've worked on the FLINT code and
# found it more accessible than Singular.
#
# The variables are listed from most significant to least significant.  We use lexicographic
# ordering to group terms together conveniently.
#
# I want 'roots' to be first in the ordering, because they're going to be substituted for,
# so making them the most significant variables groups like 'roots' terms together.
#
# I want the coeff_vars to be last in the ordering, because the system we're going to solve
# will be grouped by coordinates and ODE_vars.
#
# 'encoding' is an option that I've added to the Sage/FLINT implementation to describe
# how the polynomial terms will be written out to disk.  dexlex64(N) encodes N variables
# using 64-bit deglex encoding (see M. Gastineau, Storage of Multivariate Polynomials,
# Advanced School on Specific Algebraic Manipulators, 2007); sint64 encodes the coefficient
# as a signed 64-bit integer, so this encoding encodes each polynomial term using
# three 64-bit words.  If things don't fit, it throws an exception.

def create_polynomial_rings(alg_exts):
    global convertRing,idealRing,reduceRing,RQQ,RZZflint,R32003,convertField,num_rvars,num_cvars
    # we need to add gamma to this to make ansatz 11 (algebraic extension) work
    roots_names = list(map(varName, roots))
    alg_exts_names = [p[0] for p in alg_exts]
    num_rvars = len(alg_exts_names) + len(roots_names) + len(ODE_vars) + len(coordinates)
    num_cvars = len(coeff_vars)
    # encoding isn't used except for writing FLINT polynomials to disk with my custom code
    encoding = 'deglex64({}),deglex64({}),sint64'.format(num_rvars, num_cvars)

    # the ordering here is intended to make reduction mod alg_exts and roots easy
    Rsingular = PolynomialRing(QQ, names=tuple(flatten((alg_exts_names, roots_names, ODE_vars, coordinates, coeff_vars))),
                         order=f'lex({len(alg_exts) + len(roots_names)}), degrevlex({len(ODE_vars) + len(coordinates) + len(coeff_vars)})')

    # used with my custom option to set disk encoding
    #R = PolynomialRing(ZZ, names=tuple(flatten((alg_exts_names, roots_names, ODE_vars, coordinates, coeff_vars))),
    #                   implementation="FLINT", order='lex', encoding=encoding)
    try:
        Rflint = PolynomialRing(ZZ, names=tuple(flatten((alg_exts_names, roots_names, ODE_vars, coordinates, coeff_vars))),
                                implementation="FLINT", order='lex')
    except Exception as ex:
        print(ex)
        print('multivariate FLINT rings unavailable')
        Rflint = None

    # not only might FLINT be unavailable, it doesn't implement Groebner bases, so can't be used for reduction

    if Rflint:
        print('Using FLINT rings for convertion')
        convertRing = Rflint
    else:
        print('Using Singular rings for convertion')
        convertRing = Rsingular

    print('Using Singular rings for reduction ideal')
    idealRing = Rsingular

    #if len(roots) > 0 or len(alg_exts) > 0:
    if False:
        print('Using Singular rings for reduction')
        reduceRing = Rsingular

        # This doesn't work - Singular interface doesn't support splitting variables with the coeff field like this
        # (according to the first few lines of multi_polynomial_libsingular.pyx)
        # I'm not sure about Singular proper.  It has some kind of support for "transcendental extension of Q"
        #Rsingular1 = PolynomialRing(QQ, names=tuple(flatten((ODE_vars, coordinates, coeff_vars))))
        #Rsingular2 = PolynomialRing(FractionField(Rsingular1), names=tuple(flatten((alg_exts_names, roots_names))), order='lex')
    else:
        print('Using convertion ring for reduction')
        reduceRing = convertRing

    convertField = Frac(convertRing)

    # These are the rings used for the system of equations in the coefficients
    RQQ = PolynomialRing(QQ, names=coeff_vars)
    if Rflint:
        RZZflint = PolynomialRing(ZZ, names=coeff_vars, implementation='FLINT')
    R32003 = PolynomialRing(GF(32003), names=coeff_vars)

# we need to add gamma to this to make ansatz 11 (algebraic extension) work
def mk_ideal(R, roots, alg_exts):
    "Given a list or tuple of roots, return a ideal of ring R that reduces the global variable names of those roots"
    global reductionIdeal
    # We expect a tuple of pow's in the Symbolic Ring, so we can easily construct the minimal polynomials
    #
    # We can also take a pair of (varName, minpoly) where varName is a var in the Symbolic Ring
    # and minpoly is a polynomial in the Symbolic Ring that can be converted to R
    ideal_generators = []
    for v in roots:
        assert v.operator() is operator.pow
        Rname = R(varName(v))
        Rexpr = R(v.operands()[0])
        power = int(1/v.operands()[1])
        ideal_generators.append(Rname^power - Rexpr)
    for v,e,postsub in alg_exts:
        assert e in SR
        ideal_generators.append(R(roots_to_rs(e).subs(postsub)))
    reductionIdeal = ideal(ideal_generators)

def convert_eq_a():
    global eq_a_convertField
    # If we write this as 'eq_a_convertField = convertField(eq_a)', Sage will attempt to construct eq_a_convertField by calling
    # eq_a.numerator() and eq_a.denominator(), which will perform lots of rational function
    # math in the Symbolic Ring, which is very slow and memory intensive.  Calling it
    # like 'eq_a.polynomial(ring=convertField)' recurses through the expression tree and builds the
    # expression from the bottom up using polynomial ring operations, which are much more efficient.
    #
    # This trick (currently) only works on my development Sage, so try it and fall back on the slower way.
    try:
        eq_a_convertField = eq_a.polynomial(ring=convertField)
    except:
        print('WARNING: converting eq_a using the Symbolic Ring (this is slow)')
        eq_a_convertField = convertField(eq_a)
    print('eq_a_convertField numerator:', eq_a_convertField.numerator().number_of_terms(), 'terms')

def convertRing_to_reduceRing(element):
    # this doesn't work with my current development sage:
    #   eq_a_reduceRing_n = reduceRing(eq_a_convertField.numerator()).mod(I)
    #   eq_a_reduceRing_d = reduceRing(eq_a_convertField.denominator()).mod(I)
    # I tried converting to a string, but that hits "RecursionError: maximum recursion depth exceeded during compilation"
    #   eq_a_reduceRing_n = reduceRing(str(eq_a_convertField.numerator())).mod(I)
    #   eq_a_reduceRing_d = reduceRing(str(eq_a_convertField.denominator())).mod(I)
    # go this way instead: (works on a simple reduceRing)
    if convertRing != reduceRing:
        return reduceRing(element.dict())
    else:
        return element
    # if reduceRing is a ring over a field with convertRing variables split between the two, we need something else
    # don't bother with this code, as Singular can't support splitting variables between the ring and the coeff field
    #
    #baseRing = reduceRing.base_ring()
    #rest_term_sub = {convertRing(v):1 for v in reduceRing.variable_names()}
    #result = reduceRing(0)
    #for coeff, monomial in element:
    #    coeff_term = monomial.subs(rest_term_sub)
    #    ring_term = reduceRing(monomial / coeff_term)
    #    result += coeff * baseRing(coeff_term) * ring_term
    #return result

def reduce_mod_ideal(element, I=None):
    if I:
        # This way does the reduction in FLINT:
        # (it doesn't work in Singular, the % in Sage's Singular code is Singular's "division", which is not reduction (I'm not sure what it is)
        if 'multi_polynomial_flint' in dir(sage.rings.polynomial) \
           and type(reduceRing) == sage.rings.polynomial.multi_polynomial_flint.MPolynomialRing_flint:
            for p in I.groebner_basis():
                element %= convertRing(str(p))
            return convertRing_to_reduceRing(element)
        # This is slower if convertRing is FLINT and reduceRing is Singular; it does the reduction in Singular:
        if type(reduceRing) == sage.rings.polynomial.multi_polynomial_libsingular.MPolynomialRing_libsingular:
            return convertRing_to_reduceRing(element).mod(I)
        raise "No reduction algorithm defined for reduceRing"
    else:
        return convertRing_to_reduceRing(element)

def reduce_numerator(I=None):
    global eq_a_reduceRing_n
    eq_a_reduceRing_n = reduce_mod_ideal(eq_a_convertField.numerator(), I)
    print('eq_a_reduceRing_n:', eq_a_reduceRing_n.number_of_terms(), 'terms')

def reduce_denominator(I=None):
    global eq_a_reduceRing_d
    eq_a_reduceRing_d = reduce_mod_ideal(eq_a_convertField.denominator(), I)
    print('eq_a_convertField: denominator', eq_a_reduceRing_d.number_of_terms(), 'terms')

def timefunc(func, *args, **kwargs):
    start_time = time.perf_counter()
    retval = func(*args, **kwargs)
    end_time = time.perf_counter()
    print('{:30} {:10.2f} sec'.format(func.__name__, end_time - start_time))
    return retval

# Now expand out powers, and collect like x,y,z's terms together to
# get a system of polynomials
#
# This is a slow step, so I've tried several different ways to do it.

# Look for solutions using an approximate numerical technique

last_time = 0

# We're going from reduceRing to whatever ring is specified, or reduceRing (if not specified)

def build_system_of_equations(ring=None):
    global system_of_like_terms
    system_of_like_terms = dict()
    # for speed, build this tuple here instead of letting the subs method do it in monomial.subs
    # if my custom __evaluate function is available, use it, it's yet faster
    if '__evaluate' in dir(eq_a_reduceRing_n):
        FLINT_evaluate = tuple((n,1) for n in range(reduceRing.ngens()) if reduceRing.gen(n) in coeff_vars)
        print('Using FLINT __evaluate method')
    else:
        FLINT_evaluate = None
        non_coeff_sub = tuple(1 if reduceRing.gen(n) in coeff_vars else reduceRing.gen(n) for n in range(reduceRing.ngens()))
    pb = ProgressBar(label='build_system_of_equations ', expected_size=eq_a_reduceRing_n.number_of_terms())
    # this loop works on Singular or FLINT elements, but not other things like rings with variables in their coeff field
    for i, (coeff, monomial) in enumerate(eq_a_reduceRing_n):
        if i%100 == 99:
            pb.show(i+1)
        if FLINT_evaluate:
            non_coeff_part = monomial.__evaluate(FLINT_evaluate)
        else:
            non_coeff_part = monomial(non_coeff_sub)
        if ring:
            coeff_part = ring(monomial / non_coeff_part)
        else:
            # this cast needs to be here because otherwise the division (even though it's exact) takes us to the fraction field
            coeff_part = reduceRing(monomial / non_coeff_part)
        if (non_coeff_part) in system_of_like_terms:
            system_of_like_terms[non_coeff_part] += coeff * coeff_part
        else:
            system_of_like_terms[non_coeff_part] = coeff * coeff_part
    pb.show(i+1)
    pb.done()
    return tuple(set(system_of_like_terms.values()))

def create_eqns_RQQ():
    global eqns_RQQ, jac_eqns_RQQ
    eqns_RQQ = timefunc(build_system_of_equations, RQQ)

def create_eqns_R32003():
    global eqns_R32003
    eqns_R32003 = tuple(map(lambda arg: arg.map_coefficients(GF(32003), GF(32003)), eqns_RQQ))

def init():
    global reductionIdeal
    # convert_eq_a is the first really time consuming step
    timefunc(convert_eq_a)
    if len(roots) > 0 or len(alg_exts) > 0:
        timefunc(mk_ideal, idealRing, roots, alg_exts)
    else:
        reductionIdeal = None
    timefunc(reduce_numerator, reductionIdeal)
    # We don't use the denominator for anything, currently
    timefunc(reduce_denominator, reductionIdeal)
    timefunc(create_eqns_RQQ)
    timefunc(create_eqns_R32003)

# Solving a system of polynomials
#
# For small systems, we can just call minimal_associated_primes, but for larger systems, we wish to repeatedly
# apply two simplification steps before calling that function:
#
# 1. Factor all of the polynomials in the system of equations and build a set of systems,
# all with irreducible polynomials, that generates the same variety.
#
# 2. Find simple linear substitutions that allow a variable to be eliminated.

# Parallelized Singular polynomial factorization

def factor_eqn(eqn):
    return eqn.factor()

def parallel_factor_eqns(eqns):
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_processes) as executor:
        futures = [executor.submit(factor_eqn, eqn) for eqn in eqns]
        pb = ProgressBar(label='factor equations', expected_size=len(eqns))
        num_completed = 0
        while num_completed < len(eqns):
            concurrent.futures.wait(futures, timeout=10)
            num_completed = tuple(future.done() for future in futures).count(True)
            pb.show(num_completed)
    pb.done()
    print()
    return tuple(tuple(f for f,m in future.result()) for future in futures)

# After we factor all of the polynomials, we have a system of equations which all have to be zero,
# and each equation is a product of factors, only one of which needs to be zero to make that
# entire equation zero.  From a standpoint of logic theory, treating each factor as a logic
# variable, this is a AND-of-ORs, or a product-of-sums, a conjunctive normal form (CNF).
# We wish to convert this to disjunctive normal form (DNF), an OR-of-ANDs, sum-of-products,
# that will give us multiple systems of irreducible factors.
#
# I've tried several ways to do this, starting with native Python code and later an optimized
# C++ version of my simple algorithm.  Most recently, I've been using a dedicated logic
# optimizer, "espresso", available from https://github.com/classabbyamp/espresso-logic.

def cnf2dnf_espresso(cnf_bitsets, parallel=False):
    # convert a generator to a tuple, because we're about to iterate it twice
    cnf_bitsets = tuple(cnf_bitsets)
    cnf_bitset_lengths = set(bs.capacity() for bs in cnf_bitsets)
    assert len(cnf_bitset_lengths) == 1
    cnf_bitset_length = cnf_bitset_lengths.pop()

    # Espresso's input is a list of product terms that are logically OR-ed together to specify the ON-set in DNF.
    # How can we provide CNF input?  We take advantage of the fact that complementing CNF produces DNF, so we
    # complement all of our input bits, encoding '1' and '0' and '0' as do-not-care, then specify '-epos' to
    # swap the ON-set and OFF-set, effectively inputting the OFF-set in DNF.  See espresso man page.

    proc = subprocess.Popen(['./espresso', '-epos'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)

    # espresso reads all of its stdin before outputting anything, so there's no danger of stdin blocking here.
    proc.stdin.write(f'.i {cnf_bitset_length}\n'.encode())
    proc.stdin.write('.o 1\n'.encode())
    proc.stdin.write('.type f\n'.encode())
    for bs in cnf_bitsets:
        proc.stdin.write('{} 1\n'.format(str(bs).replace('0','-').replace('1','0')).encode())
    proc.stdin.close()

    # There is some concern that if we proc.wait() here, proc.stdout could fill with data and both processes will deadlock.
    # So we read proc.stdout first, then wait for the subprocess to terminate.
    retval = []
    for line in proc.stdout:
        line = line.decode()
        if line.startswith('.i ') or line.startswith('.o ') or line.startswith('#.phase ') or line.startswith('.p ') or line == '.e\n':
            pass
        else:
            # In principle, espresso could produce both 1s and 0s (as well as do-not-care dashes) in its output.
            # Yet I've never seen 0s in the output, and that would be very strange - it would mean that our
            # system would be true if some polynomial was NOT zero, and if that was the case, why wouldn't
            # it also be true if that polynomial was zero?  In any event, if we ever see a zero in the output,
            # it will raise an exception here.
            product_term_str, output_str = line.split(' ')
            if output_str != '1\n' or set(product_term_str) > set('1-'):
                raise RuntimeError('unexpected espresso output')
            retval.append(FrozenBitset(product_term_str.replace('-', '0')))
    proc.wait()
    if proc.returncode != 0:
        raise subprocess.CalledProcessError(proc.returncode, 'espresso')
    return retval

def cnf2dnf_external(cnf_bitsets, parallel=False):
    # we sort cnf_bitsets so that the bitsets with a single one bit come first, to speed processing in cnf2dnf
    cnf_bitsets = sorted(cnf_bitsets, key=lambda x:len(x))
    cmd = ['./cnf2dnf', '-t', str(num_processes if parallel else 1)]
    proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=sys.stderr)
    for bs in cnf_bitsets:
        proc.stdin.write(str(bs).encode())
        proc.stdin.write(b'\n')
    proc.stdin.close()
    # There is some concern that if we proc.wait() here, proc.stdout could fill with data and both processes will deadlock.
    # So we read proc.stdout first, then wait for the subprocess to terminate.
    retval = tuple(FrozenBitset(bs.decode().strip()) for bs in proc.stdout)
    proc.wait()
    if proc.returncode != 0:
        raise subprocess.CalledProcessError(proc.returncode, './cnf2dnf')
    return retval

# Which version of cnf2dnf should we use?  cnf2dnf_espresso or cnf2dnf_external

cnf2dnf = cnf2dnf_external

cnf2dnf_tests = [
    (['1'], ['1']),
    (['1', '1'], ['1']),
    (['01', '10'], ['11']),
    (['11'], ['01', '10']),
    (['11', '01'], ['01']),
    (['11', '01', '10'], ['11']),
    (['111'], ['100', '010', '001']),
    # check 5f28bf
    (['100000000', '000000001'], ['100000001']),
    # check 2c332f
    (['10000000', '00000001'], ['10000001']),
    # tests for links and chains
    (['110000000', '011000000', '001100000', '000010000', '000001100', '000000110', '000000011'],
     ['101010101', '010110101', '101011010', '101010110', '011010101', '010111010', '010110110', '011011010', '011010110']),
    (['110000000', '011000000', '001100000', '000010000', '000001100', '000000110', '000000011', '000010100'],
     ['101010101', '010110101', '101011010', '101010110', '011010101', '010111010', '010110110', '011011010', '011010110']),
    (['110000000', '011000000', '001100000', '000001100', '000010000', '000000110', '000000011', '000010100', '000001000'],
     ['101011101', '010111101', '101011010', '011011101', '010111010', '011011010']),
    # test for cover optimization
    (['1110', '0011'],
     ['1001', '0101', '0010']),
    (['01110', '00011', '11000'],
     ['01001', '10101', '10010', '01010']),
]

def test_cnf2dnf(parallel=False):
    for test in cnf2dnf_tests:
        inputs = [FrozenBitset(s) for s in test[0]]
        outputs = [FrozenBitset(s) for s in test[1]]
        actual_outputs = cnf2dnf(inputs, parallel)
        # why do we need to convert to str here for the sort to work?
        if sorted(actual_outputs, key=lambda x:str(x)) != sorted(outputs, key=lambda x:str(x)):
            print("Expected:", sorted(outputs, key=lambda x:str(x)))
            print("Actual:", sorted(actual_outputs, key=lambda x:str(x)))
            raise RuntimeError('cnf2dnf test failed')
    print('All cnf2dnf tests passed')

# Once we've built all of the systems, then we do this:
#
# consolidate_ideals(reduce(lambda a,b: a.union(b), [set(ideal(s).minimal_associated_primes()) for s in systems]))
#
# i.e, compute the minimum associated prime ideals of each system, union the results all together,
# and discard any ideals which are a subset of another ideal
#
# For hydrogen-5, this method produces the same set of five ideals that were generated by computing
# the radical and primary decomposition directly on the original system.  My hope is that this
# method will be usable for helium-16.6, where the direct approach runs out of memory, though
# it is slower for hydrogen-5.

# consolidate_ideals(list_of_ideals)
#
# For ideals, A < B if A is a subset of B
#
# We seek to represent our original ideal as an intersection of prime ideals.
# If any ideal is a strict subset of another in the set, we
# can discard the larger of the two without affecting the intersection.

def consolidate_ideals(list_of_ideals):
    consolidated_ideals = []
    for ideal in list_of_ideals:
        if any(I < ideal for I in consolidated_ideals):
            continue
        consolidated_ideals = [I for I in consolidated_ideals if not ideal < I]
        consolidated_ideals.append(ideal)
    return consolidated_ideals

#
# Helper functions for the "Pseudo-Solution of Hydrogen" paper
#

def latex_array(eqns):
    print("\\begin{array}{r}")
    for eqn in eqns:
        print(latex(eqn) + "\\\\")
    print("\\end{array}\n")

# The "simplifyIdeal" procedure in Singular's primdec.lib (primary decomposition library) checks
# for equations with simple variable substitutions, but doesn't get all linear relations.
# It's used as a preprocessing step before starting into something like the GTZ algorithm
# to compute a primary decomposition.  Let's do that step here, but also check for the
# more complicated linear relations.
#
# It finds things like v+p()=0, where p() doesn't involve v, but I also want to get
# q()v+p()=0, which can be split into two systems, one where q and p are both zero,
# and the other where v=-p/q.

try:
    # This version of simplifyIdeal (the fastest) requires the input to be a list, not a tuple,
    # and returns a modified version of that list.
    from sage.rings.polynomial.multi_polynomial_ideal_libsingular import simplifyIdeal_libsingular as simplifyIdeal
except ImportError:
    try:
        from sage.libs.singular.function_factory import ff
        singularSimplifyIdeal = ff.simplifyIdeal__lib.simplifyIdealBWB
        def simplifyIdeal(I):
            return singularSimplifyIdeal(ideal(I))
        print("Optimized simplifyIdeal not available; failing back on Singular version")
    except NameError:
        print("Neither optimized nor Singular simplifyIdealBWB available; falling back on slow Python version")
        def simplifyIdeal(I):
            # I should be a list or a tuple, not an ideal
            # returns a pair: a list of equations and a list of substitutions
            # The substitutions are equations with a simple linear term that were eliminated from the first list of equations
            simplifications = []
            for v in I[0].parent().gens():
                for p in I:
                    if p == 0:
                        pass
                    elif p/p.lc() == v:
                        #print(v, "=", 0)
                        I = tuple(map(lambda p: p.subs({v: 0}), I))
                        simplifications.append(v)
                    elif p.degree(v) == 1:
                        q,r = p.quo_rem(v)
                        if r == 0:
                            # We should pick up this case with another run through cnf2dnf
                            # print("reducible polynomial detected")
                            pass
                        elif q.is_constant() and r.number_of_terms() == 1:
                            # polynomial is qv+r; qv+r=0; replace v with -r/q
                            #print(v, "=", -r/q)
                            #start_time = time.time()
                            I = tuple(map(lambda p: p.subs({v: -r/q}), I))
                            #execution_time = time.time() - start_time
                            #print(f'subs done in {execution_time} seconds')
                            simplifications.append(q*v+r)
                            break
            return I,tuple(simplifications)

# We use simple simplifications (factoring polynomials and substituting for linear variables) to split a big
# system of polynomial equations into subsystems, each of which are then pickled and stored into a SQL
# database.  Then we'll come back and simplify each subsystem using Singular's GTZ algorithm.
#
# Each system is separated into two subsets - complex polynomials (no linear terms) and simple polynomials (a linear term in each).
# This is done to simplify the GTZ calculations, which only need to be done on the complex set, as
# once you know the solutions to the complex set, the solutions to the linear set can be easily calculated.

sql_schema='''
CREATE TYPE status AS ENUM ('queued', 'running', 'finished', 'interrupted', 'failed');

-- irreducible varieties

CREATE TABLE prime_ideals (
      identifier INTEGER GENERATED ALWAYS AS IDENTITY,
      ideal BYTEA,                -- a pickle of a list of polynomials
      degree INTEGER,             -- the maximum degree of the polynomials in the ideal
      num INTEGER
);

CREATE UNIQUE INDEX ON prime_ideals(md5(ideal));

-- systems that have been simplified with simplifyIdeal and cnf2dnf, and are ready for GTZ processing

CREATE TABLE systems (
      identifier INTEGER GENERATED ALWAYS AS IDENTITY,
      system BYTEA,               -- a pickle of a tuple pair of tuples of polynomials; the first complex, the second simple
      current_status status,
      degree INTEGER,             -- the maximum degree of the polynomials in the system
      npolys INTEGER,             -- the number of polynomials in the complex part of the system
      nvars INTEGER,              -- the number of variables in the complex part of the system
      cpu_time INTERVAL,
      memory_utilization BIGINT,
      node VARCHAR,
      pid INTEGER,
      num INTEGER                 -- the number of identical systems that have been found
);

CREATE UNIQUE INDEX ON systems(md5(system));

-- this next index doesn't help GTZ_single_thread very much
-- CREATE INDEX ON systems(current_status, identifier);
-- this index helps GTZ_single_thread at first, but after a while isn't as useful without the next one
-- we need both, it seems
CREATE INDEX ON systems(identifier);
-- this index does help GTZ_single_thread quite a bit
CREATE INDEX ON systems(identifier) where current_status = 'queued' or current_status = 'interrupted';

-- systems that have only be partially simplified

CREATE TABLE staging (
      identifier INTEGER GENERATED ALWAYS AS IDENTITY,
      stage INTEGER,              -- the original system is stage 0, each system derived from it is one integer higher
      origin INTEGER,             -- for stage 1, 0; for later stages, which identifier in the previous stage this system came from
      system BYTEA,               -- a pickle of a tuple pair of tuples of polynomials; the first complex, the second simple
      current_status status,
      node VARCHAR,
      pid INTEGER
);

-- tracking from staging to systems

CREATE TABLE systems_tracking (
      origin INTEGER,
      destination INTEGER,
      count INTEGER
);

CREATE UNIQUE INDEX ON systems_tracking(origin, destination);

-- tracking from systems to prime_ideals

CREATE TABLE prime_ideals_tracking (
      origin INTEGER,
      destination INTEGER
);

CREATE UNIQUE INDEX ON prime_ideals_tracking(origin, destination);

-- This table contains pickled rings and pickled polynomials, to keep down the size of the pickled systems.
-- We expect lots of duplicate polynomials, so we only want to store each one once.

CREATE TABLE globals (
      identifier INTEGER GENERATED ALWAYS AS IDENTITY,
      pickle BYTEA
);

-- Making "pickle" unique causes the resulting index to exceed a PostgreSQL limit, so we make the md5 hash unique instead.

CREATE UNIQUE INDEX ON globals(md5(pickle));

CREATE INDEX ON globals(identifier);

CREATE TABLE staging_stats (
      identifier INTEGER,
      node VARCHAR,
      pid INTEGER,
      unpickle_time INTERVAL,
      factor_time INTERVAL,
      cnf2dnf_time INTERVAL,
      save_global_time INTERVAL,
      simplifyIdeal_time INTERVAL,
      insert_into_systems_time INTERVAL,
      total_time INTERVAL,
      memory_utilization BIGINT
);
'''

def delete_database():
    with conn.cursor() as cursor:
        cursor.execute("DROP OWNED BY current_user")
    conn.commit()

def create_database():
    with conn.cursor() as cursor:
        cursor.execute(sql_schema)
    conn.commit()

# To keep the size of our pickled objects down, we use the "persistent data" feature of the pickle
# protocol, which allows objects to be tagged with an identifier string that is saved into
# the pickle instead of the pickled object itself.
#
# We pickle the polynomials (and the ring that the polynomials come from) and save them in
# a SQL table called 'globals', then use an integer in the table as the identifier string.
#
# We also have a UNIQUE constraint on the pickled data in 'globals' so that identical
# polynomials are only stored once.
#
# Inserting polynomials into 'globals' during a long-running transaction creates deadlocks
# on the 'globals' index if there are other processes doing the same thing.
#
# To avoid this, we do all of our 'globals' operations on a separate SQL connection (conn2)
# and commit after every polynomial is inserted.  The long-running transaction is on the
# original connection (conn).

persistent_data = {}
persistent_data_inverse = {}

def save_global(obj):
    if obj in persistent_data_inverse:
        return persistent_data_inverse[obj]
    p = persistent_pickle(obj)
    # See this stackoverflow post: https://stackoverflow.com/questions/34708509/how-to-use-returning-with-on-conflict-in-postgresql
    # for issues with the simple "ON CONFLICT DO NOTHING RETURNING identifier"
    with conn2:
        with conn2.cursor() as cursor:
            cursor.execute("INSERT INTO globals (pickle) VALUES (%s) ON CONFLICT DO NOTHING", (p,))
            cursor.execute("SELECT identifier FROM globals WHERE pickle = %s", (p,))
            id = cursor.fetchone()[0]
            persistent_data[str(id)] = obj
            persistent_data_inverse[obj] = str(id)
    return str(id)

# This routine isn't called anywhere (it's just for debugging) because the globals table is large
# and we don't want to load the whole thing into memory.  But this is the idea.

def load_globals():
    with conn2:
        with conn2.cursor() as cursor:
            cursor.execute("SELECT identifier, pickle FROM globals")
            for id, p in cursor:
                obj = pickle.loads(p)
                persistent_data[str(id)] = obj
                persistent_data_inverse[obj] = str(id)

# "persistent_data_inverse[obj]" can take a long time (on the order of ten seconds) if obj
# is a large polynomial (it's the hash that takes so long), so we used tags in place
# of the polynomials where we can.

class PersistentIdTag:
    def __init__(self, tag):
        self.tag = tag

# Note that no attempt is made to save objects that aren't already in the persistent data tables.
# If you want something saved into the globals table, you have to call save_global() explicitly.

def persistent_id(obj):
    # It's hard to tell if an arbitrary Python object is hashable
    #    See https://stackoverflow.com/a/3460725/1493790
    # The stackoverflow suggestion (try/except) is quite slow
    if isinstance(obj, PersistentIdTag):
        return obj.tag
    if isinstance(obj, sage.rings.ring.Ring) or isinstance(obj, sage.rings.polynomial.multi_polynomial.MPolynomial):
        return persistent_data_inverse.get(obj, None)
    return None

# This is pretty much how the dumps code works, except that it tries first to use an optimized version from _pickle

def persistent_pickle(val):
    src = io.BytesIO()
    p = pickle.Pickler(src)
    p.persistent_id = persistent_id
    p.dump(val)
    return src.getvalue()

def persistent_load(id):
    # We don't start a new transaction here "with conn2" because the unpickling step
    # can trigger a recursive call to persistent_load while the transaction is still open,
    # and that would fail.
    if id not in persistent_data:
        with conn2.cursor() as cursor:
            cursor.execute("SELECT pickle FROM globals WHERE identifier = %s", (int(id),))
            if cursor.rowcount == 0:
                raise pickle.UnpicklingError("Invalid persistent id")
            else:
                obj = unpickle(cursor.fetchone()[0])
                persistent_data[id] = obj
                persistent_data_inverse[obj] = id
    return persistent_data[id]

def unpickle(p):
    dst = io.BytesIO(p)
    up = pickle.Unpickler(dst)
    up.persistent_load = persistent_load
    return up.load()

# We use ProcessPool's for parallelization, and need an initializer and a done callback.

def done_callback(future):
    if future.exception():
        # I'm not sure what to do here to get a full traceback printed
        traceback.print_exception(None, future.exception(), None)

def process_pool_initializer():
    # futures can't share the original SQL connection, so create a new one
    # We DON'T want to close the existing connection, since it's still being used by the parent process
    global conn, conn2
    conn = psycopg2.connect(**postgres_connection_parameters)
    conn2 = psycopg2.connect(**postgres_connection_parameters)

def dump_bitset_to_SQL(origin, i):
    global bitsets
    global all_factors
    global simplifications
    t = tuple(all_factors[j] for j in bitsets[i])
    time1 = time.time()
    system = persistent_pickle((t,simplifications))
    time2 = time.time()
    with conn.cursor() as cursor:
        cursor.execute("INSERT INTO staging (system, origin, current_status) VALUES (%s, %s, 'queued')", (system, int(origin)))
    conn.commit()
    time3 = time.time()
    print('pickle time:', time2 - time1, 'insert time:', time3-time2, file=sys.stderr)

def save_factor_as_global(i):
    return save_global(all_factors[i])

def dropZeros(eqns):
    return tuple(e for e in eqns if e != 0)

def normalize(eqns):
    return tuple(e/e.lc() for e in eqns)

# Creating ProcessPools after all_factors and bitsets have been created (as globals) and passing an integer
# index into those lists (instead of passing one of the factors or bitsets) is done to avoid serialization
# delay in the parallel code.

def stage1and2(system, initial_simplifications, origin, stats=None):
    if origin == 0:
        stage = 1
    else:
        stage = 2
    print('simplifyIdeal')
    time1 = time.time()
    eqns,s = simplifyIdeal(list(system))
    time2 = time.time()
    if stats:
        stats['simplifyIdeal_time'] += time2 - time1
    print(len(s), 'simplifications')
    # See comment below for why we like to keep things sorted
    # We need simplifications to be a tuple because we're going to pickle it
    # simplifications is global so dump_bitset_to_SQL can access from the subprocesses in the second ProcessPool
    global simplifications
    simplifications = tuple(sorted(initial_simplifications + normalize(s)))
    # We can't save simplifications itself, since persistent_id() only works on rings and polynomials, not tuples
    for s in simplifications:
        save_global(s)
    time3 = time.time()
    if stats:
        stats['save_global_time'] += time3-time2
    eqns = normalize(dropZeros(eqns))
    if len(eqns) == 0:
        insert_into_systems(eqns, simplifications, origin, stats=stats)
        conn.commit()
        return
    if any(eqn == 1 for eqn in eqns):
        # the system is inconsistent and needs no further processing
        return
    # global for debugging purposes
    global eqns_factors
    eqns_factors = parallel_factor_eqns(eqns)
    time4 = time.time()
    if stats:
        stats['factor_time'] += time4-time3
    # all_factors is global so that the subprocesses in the next two ProcessPools can access it
    global all_factors
    # By sorting all_factors, we ensure that the systems inserted into stage2 are sorted,
    # because iterating over a FrozenBitset (in dump_bitset_to_SQL) generates integers
    # in ascending order, which are then used as indices to all_factors.
    # We like sorted systems because they help us detect duplicate systems and reduce duplicate work.
    all_factors = set(f for l in eqns_factors for f in l)
    print("Sorting", len(all_factors), "factors")
    all_factors = sorted(all_factors)

    pb = ProgressBar(label='saving factors as SQL globals', expected_size=len(all_factors))
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_processes, initializer=process_pool_initializer) as executor:
        futures = [executor.submit(save_factor_as_global, i) for i in range(len(all_factors))]
        for future in futures:
            future.add_done_callback(done_callback)
        num_completed = 0
        while num_completed < len(all_factors):
            concurrent.futures.wait(futures, timeout=1)
            num_completed = tuple(future.done() for future in futures).count(True)
            pb.show(num_completed)
    pb.done()
    time4a = time.time()
    if stats:
        stats['save_global_time'] += time4a-time4
    print()

    # We need to get the objects tagged with their persistent ids (they were only tagged in the ProcessPool
    # subprocesses), and I want to do this without having to transfer their pickled representations over a
    # process boundary (either the subprocesses or postgres).  It's precisely for this step that
    # save_global and save_factor_as_global (the function called by the future) return the persistent id.
    print('Loading persistent ids')
    all_factors_tags = []
    for i,future in enumerate(futures):
        id = future.result()
        persistent_data[id] = all_factors[i]
        persistent_data_inverse[all_factors[i]] = id
        all_factors_tags.append(PersistentIdTag(id))

    print('cnf2dnf')
    time5 = time.time()
    # bitsets is global so the subprocesses in the next ProcessPool can access it
    global bitsets
    bitsets = cnf2dnf((FrozenBitset(tuple(all_factors.index(f) for f in l), capacity=len(all_factors)) for l in eqns_factors), parallel=True)
    time6 = time.time()
    if stats:
        stats['cnf2dnf_time'] += time6-time5

    print('insert systems into SQL')
    with conn.cursor() as cursor:
        for bs in bitsets:
            t = tuple(all_factors_tags[j] for j in bs)
            system = persistent_pickle((t,simplifications))
            cursor.execute("INSERT INTO staging (system, stage, origin, current_status) VALUES (%s, %s, %s, 'queued')",
                           (system, int(stage), int(origin)))
    time7 = time.time()
    if stats:
        stats['insert_into_systems_time'] += time7-time6

def SQL_stage1(eqns):
    # To keep the size of the pickles down, we save the ring as a global since it's referred to constantly.
    save_global(eqns[0].parent())
    stage1and2(eqns, tuple(), 0)

def SQL_stage2(requested_identifier=None):
    with conn.cursor() as cursor:
        while True:
            # This post explains the subquery and the use of "FOR UPDATE SKIP LOCKED"
            # https://dba.stackexchange.com/a/69497
            if requested_identifier:
                cursor.execute("""UPDATE staging
                                  SET current_status = 'running', pid = %s, node = %s
                                  WHERE identifier = %s AND ( current_status = 'queued' OR current_status = 'interrupted' ) AND origin = 0
                                  RETURNING system, identifier""", (os.getpid(), os.uname()[1], int(requested_identifier)) )
                conn.commit()
            else:
                cursor.execute("""UPDATE staging
                                  SET current_status = 'running', pid = %s, node = %s
                                  WHERE identifier = (
                                      SELECT identifier
                                      FROM staging
                                      WHERE ( current_status = 'queued' OR current_status = 'interrupted' ) AND origin = 0
                                      ORDER BY identifier
                                      LIMIT 1
                                      FOR UPDATE SKIP LOCKED
                                      )
                                  RETURNING system, identifier""", (os.getpid(), os.uname()[1]) )
                conn.commit()
            if cursor.rowcount == 0:
                break
            pickled_system, identifier = cursor.fetchone()
            try:
                start_time = time.time()
                print('Unpickling stage 1 system', identifier)
                system, simplifications = unpickle(pickled_system)
                unpickle_time = time.time() - start_time

                # The keys in this dictionary must match column names in the 'staging_stats' SQL table
                # The keys ending in '_time' are processed differently because they correspond to SQL INTERVALs;
                # the other keys correspond to SQL INTEGERs, BIGINTs, or VARCHARs.  For the '_time' variables,
                # we store seconds in the dictionary, then convert them to datetime.timedelta's right
                # before we pass them to SQL.
                stats = {'identifier' : identifier,
                         'pid' : os.getpid(),
                         'node' : os.uname()[1],
                         'unpickle_time' : unpickle_time,
                         'factor_time' : 0,
                         'insert_into_systems_time': 0,
                         'cnf2dnf_time' : 0,
                         'save_global_time' : 0,
                         'simplifyIdeal_time' : 0}

                stage1and2(system, simplifications, identifier, stats=stats)

                cursor.execute("""UPDATE staging
                                  SET current_status = 'finished'
                                  WHERE identifier = %s""", (identifier, ))

                stats['total_time'] = time.time() - start_time
                stats['memory_utilization'] = psutil.Process(os.getpid()).memory_info().rss

                for k in stats:
                    if k.endswith('_time'):
                        stats[k] = datetime.timedelta(seconds = int(stats[k]))
                cursor.execute(f"INSERT INTO staging_stats ({','.join(stats.keys())}) VALUES %s", (tuple(stats.values()),))

                conn.commit()

            except KeyboardInterrupt:
                conn.rollback()
                cursor.execute("""UPDATE staging
                                  SET current_status = 'interrupted'
                                  WHERE identifier = %s""", (identifier,))
                cursor.execute("""DELETE FROM staging WHERE origin = %s""", (identifier,))
                conn.commit()
                raise
            except:
                conn.rollback()
                cursor.execute("""UPDATE staging
                                  SET current_status = 'failed'
                                  WHERE identifier = %s""", (identifier,))
                cursor.execute("""DELETE FROM staging WHERE origin = %s""", (identifier,))
                conn.commit()
                raise

            # keep our memory down by clearing our cached polynomials
            persistent_data.clear()
            persistent_data_inverse.clear()

# I'm experimenting with turning these on or off for efficiency
#
# We can dump polynomials to the SQL table 'globals' and only store persistent ids in the 'systems' table.
# This saves disk space because duplicate polynomials are only stored once.
#
# We can save tracking information that tracks which systems came from which stage2 systems.

dump_polynomials_to_globals_table = True
tracking = True

def insert_into_systems(system, simplifications, origin, stats=None):
    time1 = time.time()
    if dump_polynomials_to_globals_table:
        for eqn in system:
            save_global(eqn)
        for eqn in simplifications:
            save_global(eqn)
    time2 = time.time()
    if stats:
        stats['save_global_time'] += time2-time1
    p = persistent_pickle((system, simplifications))
    if len(system) == 0:
        deg = 1
        npolys = 0
        nvars = 0
    else:
        deg = max(p.degree() for p in system)
        npolys = len(system)
        nvars = len(set(v for p in system for v in p.variables()))
    with conn.cursor() as cursor:
        if tracking:
            cursor.execute("""INSERT INTO systems (system, degree, npolys, nvars, num, current_status) VALUES (%s, %s, %s, %s, 1, 'queued')
                              ON CONFLICT (md5(system)) DO UPDATE SET num = systems.num + 1
                              RETURNING identifier""",
                           (p, int(deg), int(npolys), int(nvars)))
            id = cursor.fetchone()[0]
            cursor.execute("""INSERT INTO systems_tracking (origin, destination, count) VALUES (%s, %s, 1)
                              ON CONFLICT (origin, destination) DO UPDATE SET count = systems_tracking.count + 1""",
                           (origin, id))
        else:
            cursor.execute("""INSERT INTO systems (system, degree, npolys, nvars, num, current_status) VALUES (%s, %s, %s, %s, 1, 'queued')
                              ON CONFLICT (md5(system)) DO UPDATE SET num = systems.num + 1""",
                           (p, int(deg), int(npolys), int(nvars)))
    # We don't commit until we're all done this stage2 system (the commit is in SQL_stage3_single_thread)
    # Not only does this improve efficiency, but it makes the entire processing step for one stage2 system
    # atomic, which simplifies things.  If the processing step fails or is interrupted, it has to be completely
    # re-run anyway, so in that case we just rollback the whole transaction.
    time3 = time.time()
    if stats:
        stats['insert_into_systems_time'] += time3-time2

# Stage 3 is different from stages 1 and 2 because there is no stage 4 per se.  In stage 3, we keep
# calling stage3() recursively until we've fully simplified the systems.  Selecting which stage to stop
# staging into SQL and just recurse until we're done is dependent on the complexity of the system.
# I've picked stage 3 arbitrarily because it seems to work for helium ansatz -16.6.

def stage3(system, simplifications, origin, stats=None):
    time1 = time.time()
    eqns,s = simplifyIdeal(list(system))
    time2 = time.time()
    if stats:
        stats['simplifyIdeal_time'] += time2 - time1
    simplifications = tuple(sorted(simplifications + normalize(s)))
    eqns = normalize(dropZeros(eqns))
    if len(eqns) == 0:
        return [(eqns, simplifications)]
    if any(eqn == 1 for eqn in eqns):
        # the system is inconsistent and needs no further processing
        return []
    eqns_factors = tuple(tuple(f for f,m in factor(eqn)) for eqn in eqns)
    time3 = time.time()
    if stats:
        stats['factor_time'] += time3-time2
    # sorting all_factors ensures each system in list_of_systems is sorted (see comment in stage1and2)
    all_factors = sorted(set(f for l in eqns_factors for f in l))
    dnf_bitsets = cnf2dnf(FrozenBitset(tuple(all_factors.index(f) for f in l), capacity=len(all_factors)) for l in eqns_factors)
    list_of_systems = tuple(tuple(all_factors[i] for i in bs) for bs in dnf_bitsets)
    time4 = time.time()
    if stats:
        stats['cnf2dnf_time'] += time4-time3
    if len(list_of_systems) == 1:
        return [(eqns, simplifications)]
    else:
        return [result for subsystem in list_of_systems for result in stage3(subsystem, simplifications, origin, stats=stats)]


def SQL_stage3_single_thread(requested_identifier=None):
    with conn.cursor() as cursor:
        while True:
            # This post explains the subquery and the use of "FOR UPDATE SKIP LOCKED"
            # https://dba.stackexchange.com/a/69497
            if requested_identifier:
                cursor.execute("""UPDATE staging
                                  SET current_status = 'running', pid = %s, node = %s
                                  WHERE identifier = %s AND ( current_status = 'queued' OR current_status = 'interrupted' )
                                  RETURNING system, identifier""", (os.getpid(), os.uname()[1], int(requested_identifier)) )
                conn.commit()
            else:
                cursor.execute("""UPDATE staging
                                  SET current_status = 'running', pid = %s, node = %s
                                  WHERE identifier = (
                                      SELECT identifier
                                      FROM staging
                                      WHERE current_status = 'queued' OR current_status = 'interrupted'
                                      ORDER BY identifier
                                      LIMIT 1
                                      FOR UPDATE SKIP LOCKED
                                      )
                                  RETURNING system, identifier""", (os.getpid(), os.uname()[1]) )
                conn.commit()
            if cursor.rowcount == 0:
                break
            pickled_system, identifier = cursor.fetchone()
            try:
                # The keys in this dictionary must match column names in the 'staging_stats' SQL table
                # The keys ending in '_time' are processed differently because they correspond to SQL INTERVALs;
                # the other keys correspond to SQL INTEGERs, BIGINTs, or VARCHARs.  For the '_time' variables,
                # we store seconds in the dictionary, then convert them to datetime.timedelta's right
                # before we pass them to SQL.
                stats = {'identifier' : identifier,
                         'pid' : os.getpid(),
                         'node' : os.uname()[1],
                         'unpickle_time' : 0,
                         'factor_time' : 0,
                         'insert_into_systems_time': 0,
                         'cnf2dnf_time' : 0,
                         'save_global_time' : 0,
                         'simplifyIdeal_time' : 0}

                start_time = time.time()
                print('Starting stage 2 system', identifier)
                system_pair = unpickle(pickled_system)
                stats['unpickle_time'] = time.time() - start_time

                for result in stage3(system_pair[0], system_pair[1], identifier, stats):
                    insert_into_systems(result[0], result[1], identifier, stats=stats)

                cursor.execute("""UPDATE staging
                                  SET current_status = 'finished'
                                  WHERE identifier = %s""", (identifier,))

                stats['total_time'] = time.time() - start_time
                stats['memory_utilization'] = psutil.Process(os.getpid()).memory_info().rss

                for k in stats:
                    if k.endswith('_time'):
                        stats[k] = datetime.timedelta(seconds = int(stats[k]))
                cursor.execute(f"INSERT INTO staging_stats ({','.join(stats.keys())}) VALUES %s", (tuple(stats.values()),))

                # This is the end of a typically long-running transaction that started when we set current_status = 'running'
                # and includes many INSERT statements that were generated during the call to stage3()
                conn.commit()

                print('Finished stage 2 system', identifier)
            except KeyboardInterrupt:
                conn.rollback()
                cursor.execute("""UPDATE staging
                                  SET current_status = 'interrupted'
                                  WHERE identifier = %s""", (identifier,))
                conn.commit()
                raise
            except:
                conn.rollback()
                cursor.execute("""UPDATE staging
                                  SET current_status = 'failed'
                                  WHERE identifier = %s""", (identifier,))
                conn.commit()
                raise

            # keep our memory down by clearing our cached polynomials
            persistent_data.clear()
            persistent_data_inverse.clear()

def SQL_stage3_parallel(max_workers = num_processes):
    try:
        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers, initializer=process_pool_initializer) as executor:
            futures = [executor.submit(SQL_stage3_single_thread) for _ in range(max_workers)]
            for future in futures:
                future.add_done_callback(done_callback)
            num_completed = 0
            while num_completed < max_workers:
                concurrent.futures.wait(futures, timeout=1)
                num_completed = tuple(future.done() for future in futures).count(True)
        for future in futures:
            future.result()
    except:
        # The only exception I've actually seen here is a BrokenProcessPool when my attempt to raise CalledProcessError
        # in one of the futures failed due to a TypeError because I didn't call the BrokenProcessPool constructor correctly.
        conn.rollback()
        with conn.cursor() as cursor:
            cursor.execute("""UPDATE staging
                              SET current_status = 'failed'
                              WHERE current_status = 'running' AND node = %s""", (os.uname()[1],))
        conn.commit()
        raise

def eliminateZeros(I):
    # I should be a list or a tuple of polynomials, not an ideal
    # returns a list of equations after substituting zero for any variables that appear alone in the system
    simplifications = []
    for v in I[0].parent().gens():
        for p in I:
            if p == 0:
                pass
            elif p/p.lc() == v:
                I = tuple(map(lambda p: p.subs({v: 0}), I))
                simplifications.append(v)
    return simplifications + [p for p in I if p != 0]

def GTZ_single_thread(requested_identifier=None):
    while True:
        with conn.cursor() as cursor:
            if requested_identifier:
                cursor.execute("""UPDATE systems
                                  SET current_status = 'running', pid = %s, node = %s
                                  WHERE identifier = %s AND ( current_status = 'queued' OR current_status = 'interrupted' )
                                  RETURNING system, identifier""", (os.getpid(), os.uname()[1], int(requested_identifier)) )
                conn.commit()
            else:
                cursor.execute("""UPDATE systems
                                  SET current_status = 'running', pid = %s, node = %s
                                  WHERE identifier = (
                                      SELECT identifier
                                      FROM systems
                                      WHERE current_status = 'queued' OR current_status = 'interrupted'
                                      ORDER BY identifier
                                      LIMIT 1
                                      FOR UPDATE SKIP LOCKED
                                      )
                                  RETURNING system, identifier""", (os.getpid(), os.uname()[1]) )
                conn.commit()
            if cursor.rowcount == 0:
                break
            pickled_system, identifier = cursor.fetchone()
            try:
                start_time = time.time()
                print('Starting system', identifier)
                system, simplifications = unpickle(pickled_system)
                if len(system) == 0:
                    subsystems = tuple((eliminateZeros(simplifications), ))
                else:
                    minimal_primes = ideal(system).minimal_associated_primes()
                    subsystems = tuple(eliminateZeros(mp.gens() + simplifications) for mp in minimal_primes)
                for ss in subsystems:
                    for p in ss:
                        save_global(p)
                    degree = max(p.degree() for p in ss)
                    p = persistent_pickle(sorted(ss))
                    cursor.execute("""INSERT INTO prime_ideals (ideal, degree, num) VALUES (%s, %s, 1)
                                      ON CONFLICT (md5(ideal)) DO UPDATE SET num = prime_ideals.num + 1
                                      RETURNING identifier""",
                                   (p, int(degree)))
                    id = cursor.fetchone()[0]
                    cursor.execute("""INSERT INTO prime_ideals_tracking (origin, destination) VALUES (%s, %s)""",
                                   (identifier, id))
                memory_utilization = psutil.Process(os.getpid()).memory_info().rss
                cpu_time = datetime.timedelta(seconds = time.time() - start_time)
                cursor.execute("""UPDATE systems
                                  SET current_status = 'finished',
                                      cpu_time = %s,
                                      memory_utilization = %s
                                  WHERE identifier = %s""", (cpu_time, memory_utilization, identifier))
                conn.commit()
                print('Finished system', identifier)
            except KeyboardInterrupt:
                conn.rollback()
                cursor.execute("""UPDATE systems
                                  SET current_status = 'interrupted'
                                  WHERE identifier = %s""", (identifier,))
                conn.commit()
                raise
            except:
                conn.rollback()
                cursor.execute("""UPDATE systems
                                  SET current_status = 'failed'
                                  WHERE identifier = %s""", (identifier,))
                conn.commit()
                raise

def GTZ_parallel(max_workers = num_processes):
    try:
        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers, initializer=process_pool_initializer) as executor:
            futures = [executor.submit(GTZ_single_thread) for _ in range(max_workers)]
            for future in futures:
                future.add_done_callback(done_callback)
            num_completed = 0
            while num_completed < max_workers:
                concurrent.futures.wait(futures, timeout=1)
                num_completed = tuple(future.done() for future in futures).count(True)
        for future in futures:
            future.result()
    except:
        # The only exception I've actually seen here is a BrokenProcessPool when my attempt to raise CalledProcessError
        # in one of the futures failed due to a TypeError because I didn't call the BrokenProcessPool constructor correctly.
        conn.rollback()
        with conn.cursor() as cursor:
            cursor.execute("""UPDATE systems
                              SET current_status = 'failed'
                              WHERE current_status = 'running' AND node = %s""", (os.uname()[1],))
        conn.commit()
        raise

def load_prime_ideals():
    retval = []
    with conn.cursor() as cursor:
        cursor.execute("SELECT ideal FROM prime_ideals")
        for sys in cursor:
            retval.append(ideal(unpickle(sys[0])))
    return retval

# Various utility functions for debugging the SQL database from the command line

def load_systems():
    retval = []
    with conn.cursor() as cursor:
        cursor.execute("SELECT system FROM systems;")
        for pickled_system in cursor:
            retval(unpickle(pickled_system[0]))
    return retval

def list_systems():
    with conn.cursor() as cursor:
        cursor.execute("SELECT system FROM systems WHERE current_status = 'finished'")
        for sys in cursor:
            print(unpickle(sys[0]))

def load_prime_ideal(identifier):
    with conn:
        with conn.cursor() as cursor:
            cursor.execute("SELECT ideal FROM prime_ideals WHERE identifier = %s", (int(identifier),))
            return unpickle(cursor.fetchone()[0])

def load_stage1(identifier):
    with conn:
        with conn.cursor() as cursor:
            cursor.execute("SELECT system FROM staging WHERE identifier = %s", (int(identifier),))
            return unpickle(cursor.fetchone()[0])

def get_system_sizes():
    with conn.cursor() as cursor:
        cursor.execute("SELECT identifier, length(system) FROM systems ORDER BY length(system)")
        return [v for v in cursor]

def SQL_stage2_reset():
    with conn:
        with conn.cursor() as cursor:
            cursor.execute("DELETE FROM staging WHERE origin != 0")
            cursor.execute("DELETE FROM staging_stats")
            cursor.execute("UPDATE staging SET current_status = 'queued', pid = NULL, node = NULL")

def SQL_stage3_reset():
    with conn:
        with conn.cursor() as cursor:
            cursor.execute("DELETE FROM systems")
            cursor.execute("DELETE FROM systems_tracking")
            cursor.execute("DELETE FROM staging_stats USING staging WHERE staging_stats.identifier = staging.identifier AND staging.origin != 0")
            cursor.execute("UPDATE staging SET current_status = 'queued', pid = NULL, node = NULL WHERE origin != 0")

def SQL_GTZ_reset():
    with conn:
        with conn.cursor() as cursor:
            cursor.execute("""UPDATE systems
                              SET current_status = 'queued', pid = NULL, node = NULL
                              WHERE current_status != 'queued'""")
            cursor.execute("DELETE FROM prime_ideals")
            cursor.execute("DELETE FROM prime_ideals_tracking")

